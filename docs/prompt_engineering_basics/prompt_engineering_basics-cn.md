**• What is the difference between Predictive/Discriminative AI and Generative AI?**  

Predictive/Discriminative AI and Generative AI represent fundamentally distinct approaches in artificial intelligence:  

1.  **Predictive/Discriminative AI**  
    -   **Objective:** Classifies or predicts labels for given input data. It learns the boundary between classes or predicts outcomes based on patterns in existing data.  
    -   **Mechanism:** Models the conditional probability `P(Y|X)` (probability of output `Y` given input `X`).  
    -   **Output Type:** Discrete labels (e.g., "cat" vs. "dog") or continuous values (e.g., house price prediction).  
    -   **Examples:**  
        -   Spam email detection (input: email text; output: "spam" or "not spam").  
        -   Medical diagnosis (input: patient symptoms; output: disease probability).  
    -   **Architectures:** Logistic regression, Support Vector Machines (SVMs), and Convolutional Neural Networks (CNNs) for image classification.  

2.  **Generative AI**  
    -   **Objective:** Creates new data samples that resemble training data. It learns the underlying distribution of the data to generate novel content.  
    -   **Mechanism:** Models the joint probability `P(X,Y)` or learns data distribution `P(X)` to synthesize new `X`.  
    -   **Output Type:** Complex data structures like text, images, audio, or video.  
    -   **Examples:**  
        -   Image generation (e.g., DALL·E creating a painting of a "robot surfing").  
        -   Text generation (e.g., ChatGPT writing an essay on climate change).  
    -   **Architectures:** Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models like GPT.  

**Key Differences:**  
- Discriminative AI **distinguishes** between existing data categories; Generative AI **creates** new data.  
- Discriminative models answer "What is this?"; generative models answer "What could this be?"  

---

**• What is an LLM, and how are LLMs trained?**  

**LLM (Large Language Model)**: An artificial intelligence system designed to understand, generate, and manipulate human language. It leverages deep learning architectures (typically transformers) trained on massive text corpora.  

**Training Process**:  

1.  **Pre-training (Foundation Modeling)**:  
    -   **Objective:** Learn general language patterns, grammar, facts, and reasoning abilities.  
    -   **Data:** Trained on terabytes of unlabeled text from diverse sources (e.g., books, websites, scientific papers).  
    -   **Task:** Predict the next token in a sequence (autoregressive modeling) or fill masked tokens.  
    -   **Hardware:** Requires thousands of GPUs/TPUs for weeks or months.  
    -   **Architecture:** Transformer neural networks with self-attention mechanisms, scaling to billions of parameters (e.g., GPT-4: ~1.7 trillion parameters).  

2.  **Fine-tuning**:  
    -   **Objective:** Specialize the pre-trained model for specific tasks (e.g., chatbots, code generation).  
    -   **Data:** Smaller, task-specific labeled datasets (e.g., human-curated Q&A pairs).  
    -   **Techniques:**  
        -   *Supervised Fine-tuning (SFT):* Adjust model weights using labeled examples.  
        -   *Reinforcement Learning from Human Feedback (RLHF):* Optimize responses based on human preferences.  

3.  **Alignment (Optional)**:  
    -   Refines model behavior to align with human values (e.g., avoiding harmful outputs).  

---

**• What is a token in the language model?**  

A **token** is the smallest unit of data processed by a language model. It represents fragmented text or subwords:  

1.  **Definition**:  
    -   Tokens are generated by a *tokenizer* that splits raw text into manageable chunks.  
    -   Example: "ChatGPT is powerful!" → Tokens: ["Chat", "G", "PT", " is", " powerful", "!"].  

2.  **Types**:  
    -   *Words:* Common terms (e.g., "apple").  
    -   *Subwords:* Parts of complex words (e.g., "##ing" for "running").  
    -   *Characters/Punctuation:* Individual letters/symbols (e.g., "!", "?").  

3.  **Significance**:  
    -   **Cost Calculation:** LLM pricing (e.g., OpenAI API) bills per token (input + output).  
    -   **Context Limits:** Models have fixed token capacities (e.g., 128K tokens for GPT-4 Turbo).  
    -   **Performance:** Token choice impacts model efficiency; subword tokenization (e.g., Byte-Pair Encoding) balances vocabulary size and out-of-vocabulary robustness.  

---

**• How to estimate the cost of running SaaS-based and Open Source LLM models?**  

### **SaaS-Based LLMs (e.g., OpenAI, Anthropic)**  
1.  **Input/Output Tokens**:  
    -   Cost = (Input tokens × Price per input token) + (Output tokens × Price per output token).  
    -   Example (GPT-4 Turbo):  
        - Input: $10/million tokens; Output: $30/million tokens  
        - Query: 5K input tokens + 2K output tokens → Cost: `(5 × $0.01) + (2 × $0.03) = $0.11`  

2.  **API Call Fees**:  
    -   Additional charges per request (e.g., $0.005/call for Claude Opus).  

3.  **Volume Tiers**:  
    -   Discounts at high usage (e.g., >10M tokens/month).  

4.  **Tool Integration**:  
    -   Fees for features like retrieval-augmented generation (RAG).  

### **Open-Source LLMs (e.g., LLaMA 3, Mistral)**  
1.  **Infrastructure Costs**:  
    -   *Cloud Hosting (AWS/GCP/Azure):*  
        - GPU Instances: e.g., NVIDIA A100 ($1.50–$4.00/hour) or H100 ($8–$12/hour).  
        - Memory/Storage: ~$0.10/GB/month for VRAM.  
    -   *On-Premises:* Upfront hardware cost + electricity/maintenance.  

2.  **Inference Costs**:  
    -   Cost per token = (Hardware cost per second) / (Tokens generated per second).  
    -   Example:  
        - A100 generates 50 tokens/sec at $3.00/hour → Cost: `$0.0000167/token`.  

3.  **Deployment Factors**:  
    -   *Throughput Requirements:* Higher queries/second demand more GPUs.  
    -   *Quantization:* 4-bit models reduce VRAM needs by 70% but may lower accuracy.  
    -   *Batching:* Grouping requests improves GPU utilization.  

4.  **Total Cost of Ownership (TCO)**:  
    -   Include engineering time for deployment, monitoring, and updates.  

### **Key Cost Drivers**  
- **Model Size:** Larger models (e.g., 70B parameters) require expensive GPUs.  
- **Query Complexity:** Long-context prompts increase token counts.  
- **Scalability:** SaaS scales automatically; open-source needs manual scaling.  
- **SaaS vs. Open-Source Trade-off:**  
    - *SaaS:* Lower upfront cost, higher marginal cost per token.  
    - *Open-Source:* High initial setup, cost-efficient at large scale.  

**• Explain the Temperature parameter and how to set it.**  

The **Temperature** parameter controls the randomness of token generation in LLMs:  

1.  **Mechanism**:  
    -   Adjusts the softmax distribution of token probabilities:  
        `Adjusted_Probability = exp(logit / T) / sum(exp(logit_i / T))`  
        where `T` = temperature, `logit` = raw model output.  
    -   **Low temperature (`T < 1.0`)**:  
        - Amplifies high-probability tokens → output becomes **deterministic and conservative**.  
        - Example: For tokens ["rain" (80%), "snow" (15%), "hail" (5%)] at T=0.5 → ["rain" (96%), "snow" (3.5%), "hail" (0.5%)].  
    -   **High temperature (`T > 1.0`)**:  
        - Flattens probability distribution → output becomes **creative and diverse**.  
        - Same tokens at T=2.0 → ["rain" (62%), "snow" (27%), "hail" (11%)].  

2.  **How to Set It**:  
    -   **T ≈ 0.0 - 0.5**: Ideal for factual Q&A, code generation. Reduces hallucinations.  
    -   **T ≈ 0.7 - 1.0**: Default for balanced creativity/accuracy (e.g., ChatGPT).  
    -   **T > 1.0**: Use for creative writing, brainstorming (may reduce coherence).  
    -   **T ≈ 0** = Deterministic mode (always picks highest-probability token).  

3.  **Calibration Tips**:  
    -   Start at `T=0.7` and increase if outputs are too repetitive.  
    -   Decrease if responses are illogical or hallucinating.  
    -   Combine with `top_p`/`top_k` for finer control.  

---

**• What are different decoding strategies for picking output tokens?**  

Decoding strategies determine how LLMs select output tokens from probability distributions:  

1.  **Greedy Decoding**:  
    -   Always selects the token with the highest probability.  
    -   *Pros:* Fast, deterministic.  
    -   *Cons:* Leads to repetitive loops (e.g., "The cat the cat the cat...").  

2.  **Beam Search**:  
    -   Maintains `k` (beam width) candidate sequences per step.  
    -   At each step, generates `k` extensions for all candidates, keeping top `k` sequences.  
    -   *Pros:* Better coherence for fixed-length outputs (e.g., translation).  
    -   *Cons:* Computationally heavy; causes generic outputs.  

3.  **Top-k Sampling**:  
    -   Samples only from the top `k` most probable tokens.  
    -   Example: With `k=30`, only tokens in the top 30 by probability are considered.  
    -   *Use case:* Balances creativity and coherence.  

4.  **Top-p (Nucleus) Sampling**:  
    -   Samples from the smallest set of tokens where cumulative probability ≥ `p`.  
    -   Example: `p=0.9` → selects tokens ordered by probability until the sum exceeds 90%.  
    -   *Advantage:* Adapts to varying probability distributions (avoids rigid `k` size).  

5.  **Typical Sampling**:  
    -   Selects tokens based on similarity to the entropy of the distribution.  
    -   *Benefit:* Mitigates over-representation of high-probability tokens.  

6.  **Contrastive Search**:  
    -   Penalizes tokens similar to previous context via:  
        `next_token = argmax{(1-α)*logit - α*max_similarity(token, context)}`  
    -   *Effect:* Reduces repetition while maintaining coherence.  

---

**• What are different ways you can define stopping criteria in large language models?**  

Stopping criteria determine when token generation ends:  

1.  **End-of-Sequence (EOS) Token**:  
    -   Models output a special token (e.g., `<|endoftext|>`) to signal completion.  
    -   Automatically terminates generation upon producing this token.  

2.  **Maximum Token Limit**:  
    -   Hard cutoff after `N` tokens (includes input + output tokens).  
    -   *Example:* Set `max_tokens=500` in OpenAI API.  

3.  **Stop Sequences**:  
    -   Predefined strings that halt generation when detected in output.  
    -   *Example:* Stopping at `"\n###"` to prevent generating beyond a header.  

4.  **Length-Based Termination**:  
    -   Stop after generating `M` tokens (output-only count).  
    -   Differs from `max_tokens` by ignoring input length.  

5.  **Probability Threshold**:  
    -   Terminate if the probability of the next token falls below a set value.  
    -   Rarely used due to unpredictability.  

6.  **Hybrid Approaches**:  
    -   Combine EOS + `max_tokens` (e.g., stop at EOS or after 200 tokens).  

---

**• How to use stop sequences in LLMs?**  

Stop sequences are user-defined strings that halt text generation:  

1.  **Implementation**:  
    -   **APIs**: Pass as a list parameter.  
        ```python
        # OpenAI API example
        response = openai.Completion.create(
          model="gpt-4",
          prompt="Explain quantum mechanics:\n",
          stop=["\n\n", "### Footnotes"]  # Stops at double newline or footnote header
        )
        ```  
    -   **Self-hosted Models**: Tokenizers convert sequences to token IDs; generation stops if these token sequences appear.  

2.  **Use Cases**:  
    -   **Multi-turn Dialogs**: Stop after `"User:"` to prevent the model from impersonating users.  
    -   **Structured Outputs**: Terminate after `"```"` to avoid generating beyond a code block.  
    -   **Factual Responses**: Halt at `"[Citation Needed]"` to restrict unsupported claims.  

3.  **Best Practices**:  
    -   **Specificity**: Use sequences unlikely to appear mid-response (e.g., `"<END>"`).  
    -   **Multiple Sequences**: Define backups (e.g., `["</response>", "\n###"]`).  
    -   **Escape Special Characters**: For regex-like symbols (e.g., `\n` for newline).  
    -   **Testing**: Verify sequences align with tokenizer behavior (e.g., `"\n"` vs. newline token).  

4.  **Limitations**:  
    -   Sequences **must appear verbatim** in output; partial matches are ignored.  
    -   Stopping may occur mid-sequence if token boundaries misalign.  
    -   Requires trial-and-error for complex terminators (e.g., natural conversation endings).  

**• Explain the basic structure of prompt engineering.**  

Prompt engineering involves designing inputs (prompts) to guide LLMs to produce desired outputs. Its core structure comprises three components:  

1.  **Instruction**:  
    -   Clear task specification telling the model *what to do*.  
    -   Example:  
        ```  
        "Translate the following English text to French:"  
        ```  

2.  **Context**:  
    -   Supporting information that narrows the solution space.  
    -   Includes:  
        - *Input data*: Text to process  
        - *Constraints*: Output rules (e.g., "50 words max")  
        - *Role definition*: "Act as a cybersecurity expert"  

3.  **Output Formatting**:  
    -   Explicit specification of how results should be structured.  
    -   Examples:  
        - JSON: `{"translation": "..."}`  
        - Markdown headers: `## Summary`  
        - Delimiters: `Answer inside <response></response> tags`  

**Complete Example**:  
 ```
You are an AI nutritionist. Calculate daily calories for a 30-year-old woman (165cm, 65kg) who exercises 3x/week.
Use the Mifflin-St Jeor formula.
Output: {"calories": number, "formula": "Mifflin-St Jeor"}
 ```


---

**• Explain in-context learning.**  

**In-context learning (ICL)** enables LLMs to learn new tasks during inference by providing demonstrations *within the prompt itself* without weight updates.  

1.  **Mechanism**:  
    -   **Demonstration**: 2–5 input-output pairs showcasing the task.  
    -   **Query**: New input needing prediction.  
    -   *Prompt Structure*:  
        ```  
        Input: "3+5" → Output: "8"  
        Input: "10-2" → Output: "8"  
        Input: "4 * 6" → Output: ?  
        ```  

2.  **Key Properties**:  
    -   **Zero-shot**: No examples (`"Translate 'hello' to French:"`)  
    -   **One-shot**: One example  
    -   **Few-shot**: Multiple examples (typically 3–5)  
    -   Models learn pattern recognition from demonstrations using attention mechanisms.  

3.  **Benefits**:  
    -   Adapts to unseen tasks instantly  
    -   Reduces need for fine-tuning  
    -   Enables meta-learning (e.g., teaching formatting rules)  

---

**• Explain type of prompt engineering**  

Prompt engineering strategies vary by task complexity and control needs:  

1.  **Instructional Prompts**:  
    -   Direct commands: `"Summarize this article in 3 bullet points:"`  

2.  **Role-Playing Prompts**:  
    -   Assigns a persona: `"As Shakespeare, write a sonnet about AI."`  

3.  **Chain-of-Thought (CoT)**:  
    -   Forces step-by-step reasoning:  
        ```  
        Q: A bat costs $10 more than a ball. Total cost = $110. Ball cost?  
        A: Let ball = $x. Bat = $x + $10. Total: x + (x+10) = 110 →  
        2x + 10 = 110 → 2x = 100 → x = 50. Ball costs $50.  
        ```  

4.  **Self-Consistency Prompts**:  
    -   Generates multiple reasoning paths and votes for best answer  

5.  **Tree-of-Thought (ToT)**:  
    -   Breaks problems into intermediate "thought" nodes for complex tasks  

6.  **Automatic Prompt Engineering (APE)**:  
    -   Uses LLMs to generate/optimize prompts ("Promptception")  

---

**• What are some of the aspects to keep in mind while using few-shot prompting?**  

1.  **Example Quality**:  
    -   Use clean, unambiguous demonstrations.  
    -   Avoid mislabeled/contradictory examples.  

2.  **Example Quantity**:  
    -   Balance between under-specification (too few) and token waste (too many).  
    -   *Optimal*: 2–5 examples for most tasks.  

3.  **Example Order**:  
    -   Place most relevant examples near the query.  
    -   Random order can cause >20% accuracy drops.  

4.  **Format Consistency**:  
    -   Maintain identical input/output structures.  
    -   Example:  
        ```  
        # Good:  
        English: "Hello" → French: "Bonjour"  
        English: "Goodbye" → French: "Au revoir"  
        ```  
        ```  
        # Bad:  
        "Hello" -> "Bonjour"  
        French translation of "Goodbye" is "Au revoir"  
        ```  

5.  **Token Efficiency**:  
    -   Truncate examples exceeding context windows.  
    -   For long-context tasks (e.g., doc summary), prioritize key content.  

6.  **Domain Alignment**:  
    -   Medical prompts need medical examples – generic ones reduce accuracy by 30–60%.  

7.  **Edge Cases**:  
    -   Include rare scenarios (e.g., "Translate: 'I am' → French") to reduce failures.  

**• What are certain strategies to write good prompts?**  

Effective prompt engineering combines art and science. Key strategies:  

1.  **Clarity & Specificity**:  
    -   Bad: "Write about space"  
    -   Good: "Write a 200-word engaging paragraph about NASA's Artemis mission objectives for high school students"  

2.  **Persona Assignment**:  
    ```  
    "As a senior software engineer with 15 years of React experience,  
    review this code snippet for accessibility best practices:"  
    ```  

3.  **Structured Output Formatting**:  
    -   Use JSON, XML, or markdown templates:  
        ```  
        Output in JSON: {"advantages": [item1, item2], "disadvantages": [item1]}  
        ```  

4.  **Negative Constraints**:  
    ```  
    "Explain quantum entanglement without using mathematical equations  
    or technical jargon beyond high-school physics level."  
    ```  

5.  **Multi-shot Demonstrations**:  
    ```  
    Input: "Sales grew 15% YoY" → Output: "Positive trend (↑15% YoY)"  
    Input: "Profits fell 8%" → Output: "Negative trend (↓8%)"  
    Input: "User engagement dropped 3% QoQ"  
    ```  

6.  **Chain of Thought (CoT) Triggering**:  
    ```  
    "Think step by step before answering: What's the environmental impact  
    of electric vehicle batteries from mining to disposal?"  
    ```  

7.  **Delimiters for Precision**:  
    ```  
    Summarize text between <!--START--> and <!--END-->:  
    <!--START--> [Text here] <!--END-->  
    ```  

---

**• What is hallucination, and how can it be controlled using prompt engineering?**  

**Hallucination**: LLMs generate factually incorrect or unfounded content while presenting it confidently.  

**Causes**:  
- Training data gaps/errors  
- Overgeneralization  
- High-temperature sampling  

**Prompt Engineering Controls**:  

1.  **Grounding Constraints**:  
    ```  
    "Answer using ONLY information from this document: [paste text].  
    If unsure, say 'I lack sufficient context'."  
    ```  

2.  **Citation Requirements**:  
    ```  
    "For each claim about climate change effects, cite page numbers from the IPCC AR6 report."  
    ```  

3.  **Uncertainty Acknowledgment**:  
    ```  
    "First check if this claim is scientifically verified. If evidence is weak,  
    say 'This requires further verification'."  
    ```  

4.  **Verification Steps**:  
    ```  
    "Before finalizing the answer:  
    1. Identify factual claims  
    2. Cross-validate with embedded sources  
    3. Flag unverified statements"  
    ```  

5.  **Confidence Scoring**:  
    ```  
    "Assign 1-5 confidence scores to each statement:  
    ✅(5=peer-reviewed) → ❓(1=anecdotal)"  
    ```  

6.  **Source Exclusion**:  
    ```  
    "Do NOT reference sources older than 2020 for AI ethics guidelines."  
    ```  

---

**• How to improve the reasoning ability of LLM through prompt engineering?**  

1.  **Explicit Chain-of-Thought (CoT)**:  
    ```  
    "Produce a step-by-step reasoning chain before answering:  
    Step 1: Break down the problem  
    Step 2: Identify key variables  
    Step 3: Apply logical operations"  
    ```  

2.  **Multi-Perspective Analysis**:  
    ```  
    "Evaluate from 3 angles:  
    a) Economic feasibility  
    b) Social impact  
    c) Technical scalability  
    Then synthesize conclusions"  
    ```  

3.  **Socratic Questioning**:  
    ```  
    "Generate 5 critical questions about this hypothesis before assessing validity"  
    ```  

4.  **Rubric-Based Evaluation**:  
    ```  
    "Score this argument using:  
    Logic consistency (0-5)  
    Evidence quality (0-5)  
    Counterargument addressing (0-5)"  
    ```  

5.  **Iterative Refinement**:  
    ```  
    "First draft: [Initial solution]  
    Second pass: Identify flaws  
    Final version: Corrected approach"  
    ```  

6.  **Analogical Reasoning**:  
    ```  
    "Solve this physics problem by comparing it to hydraulic systems,  
    highlighting 3 parallel principles"  
    ```  

---

**• How to improve LLM reasoning if your COT prompt fails?**  

When standard Chain-of-Thought fails, escalate to advanced techniques:  

1.  **Tree of Thoughts (ToT)**:  
    ```  
    "Explore 3 distinct solution approaches to this calculus problem:  
    Approach 1: Integration by parts  
    Approach 2: Trigonometric substitution  
    Approach 3: Series expansion  
    Compare outcomes and select best path"  
    ```  

2.  **Self-Consistency Prompting**:  
    ```  
    "Generate 5 alternative reasoning paths for this ethical dilemma.  
    Then vote for the most logically consistent solution."  
    ```  

3.  **Hybrid Verification**:  
    ```  
    "Reason step-by-step ➔ Code the solution in Python ➔ Verify results numerically ➔ Reconcile discrepancies"  
    ```  

4.  **External Tool Integration**:  
    ```  
    "When calculating probabilities:  
    Step 1: Reason theoretically  
    Step 2: Verify using embedded calculator  
    Step 3: Resolve conflicts"  
    ```  

5.  **Reflective Debriefing**:  
    ```  
    "After solving, perform failure analysis:  
    a) What assumptions were risky?  
    b) What alternative methods exist?  
    c) How might domain experts critique this?"  
    ```  

6.  **Constrained Brainstorming**:  
    ```  
    "Generate 3 solutions under these constraints:  
    • Budget < $100K  
    • Timeline < 3 months  
    • Must use existing infrastructure  
    Then select the optimal path"  
    ```  