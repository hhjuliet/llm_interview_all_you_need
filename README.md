[ä¸­æ–‡](#section-zh) | [English](#section-en)

<a name="section-zh"></a>

<!-- ä¸­è‹±æ–‡åˆ†æ å¸ƒå±€ -->

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
<div style="grid-column: 1;">

<!-- ä¸­æ–‡éƒ¨åˆ† -->

# LLM é¢è¯•å®Œå…¨æŒ‡å— (llm_interview_all_you_need)

ğŸ“š æ”¶é›†é¡¶çº§å…¬å¸ LLM é¢è¯•é«˜é¢‘é—®é¢˜ï¼Œæ¬¢è¿ç¤¾åŒºå…±åˆ›ï¼
ğŸš€ æ­£åœ¨ä¸æ–­æ•´ç†æ¥è‡ª Google/OpenAI/Meta/Anthropic ç­‰å…¬å¸çš„çœŸé¢˜ï¼Œæ¶µç›– RAGã€å¾®è°ƒã€éƒ¨ç½²ç­‰æ ¸å¿ƒé¢†åŸŸã€‚

## ç›®å½•

1. [æç¤ºå·¥ç¨‹ä¸ LLM åŸºç¡€](#prompt-engineering--basics-of-llm)
2. [æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)](#retrieval-augmented-generation-rag)
3. [æ–‡æ¡£æ•°å­—åŒ–ä¸åˆ†å—](#document-digitization--chunking)
4. [åµŒå…¥æ¨¡å‹](#embedding-models)
5. [å‘é‡æ•°æ®åº“åŸç†](#internal-working-of-vector-databases)
6. [é«˜çº§æœç´¢ç®—æ³•](#advanced-search-algorithms)
7. [è¯­è¨€æ¨¡å‹åŸç†](#language-models-internal-working)
8. [ç›‘ç£å¾®è°ƒ (SFT)](#supervised-fine-tuning-of-llm)
9. [åå¥½å¯¹é½ (RLHF/DPO)](#preference-alignment-rlhfdpo)
10. [è¯„ä¼° LLM ç³»ç»Ÿ](#evaluation-of-llm-system)
11. [å¹»è§‰æ§åˆ¶æŠ€æœ¯](#hallucination-control-techniques)
12. [LLM éƒ¨ç½²æ–¹æ¡ˆ](#deployment-of-llm)
13. [æ™ºèƒ½ä½“ç³»ç»Ÿ](#agent-based-system)
14. [æç¤ºæ³¨å…¥æ”»é˜²](#prompt-hacking)
15. [ç»¼åˆè¯é¢˜](#miscellaneous)
16. [å®æˆ˜æ¡ˆä¾‹](#case-studies)

---

### æç¤ºå·¥ç¨‹ä¸ LLM åŸºç¡€

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **ç”Ÿæˆå¼ AI ä¸åˆ¤åˆ«å¼ AI çš„æ ¸å¿ƒåŒºåˆ«ï¼Ÿ**
- **è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæµç¨‹è§£æ**
- **Temperature å‚æ•°çš„ä½œç”¨ä¸è®¾ç½®åŸåˆ™**
- **LLM è§£ç ç­–ç•¥æ¯”è¾ƒåˆ†æ**
- **å¦‚ä½•å®šä¹‰å¤§è¯­è¨€æ¨¡å‹çš„åœæ­¢æ¡ä»¶ï¼Ÿ**
- **åœæ­¢åºåˆ—åœ¨ LLM ä¸­çš„åº”ç”¨æ–¹æ³•**
- **æç¤ºå·¥ç¨‹çš„åŸºæœ¬ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ**
- **ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶è§£æ**
- **æç¤ºå·¥ç¨‹çš„ç±»å‹ä¸å®æ–½æ–¹æ³•**
- **å°‘æ ·æœ¬æç¤ºçš„å…³é”®æ³¨æ„äº‹é¡¹**
- **ç¼–å†™é«˜è´¨é‡æç¤ºçš„æœ‰æ•ˆç­–ç•¥**
- **å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹æ§åˆ¶ LLM å¹»è§‰**
- **ä½¿ç”¨æç¤ºå·¥ç¨‹å¢å¼º LLM æ¨ç†èƒ½åŠ›**
- **å½“æ€ç»´é“¾(CoT)æç¤ºå¤±æ•ˆæ—¶çš„æ”¹è¿›æ–¹æ³•**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/prompt_engineering_basics)

</details>

### æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å¦‚ä½•æé«˜ LLM è¾“å‡ºçš„å‡†ç¡®æ€§å’Œå¯é æ€§**
- **RAG å·¥ä½œæœºåˆ¶è¯¦ç»†è§£æ**
- **ä½¿ç”¨ RAG ç³»ç»Ÿçš„ä¸»è¦ä¼˜åŠ¿**
- **å¾®è°ƒ vs RAG çš„é€‰æ‹©æ ‡å‡†**
- **ç§æœ‰æ•°æ®å®šåˆ¶åŒ– LLM çš„æ¶æ„æ¨¡å¼**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/rag_systems)

</details>

### æ–‡æ¡£æ•°å­—åŒ–ä¸åˆ†å—

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **åˆ†å—çš„åŸºæœ¬æ¦‚å¿µä¸å¿…è¦æ€§**
- **å½±å“åˆ†å—å¤§å°çš„å…³é”®å› ç´ **
- **ä¸åŒç±»å‹çš„åˆ†å—æ–¹æ³•æ¯”è¾ƒ**
- **å¯»æ‰¾æœ€ä½³åˆ†å—å¤§å°çš„ç­–ç•¥**
- **å¤æ‚æ–‡æ¡£ï¼ˆå¹´æŠ¥ï¼‰çš„æ•°å­—åŒ–å¤„ç†æ–¹æ¡ˆ**
- **è¡¨æ ¼å¤„ç†çš„æœ€ä½³å®è·µ**
- **å¤§å‹è¡¨æ ¼çš„æ£€ç´¢ä¼˜åŒ–æ–¹æ³•**
- **åˆ—è¡¨é¡¹çš„åˆ†å—å¤„ç†æŠ€æœ¯**
- **ç”Ÿäº§çº§æ–‡æ¡£å¤„ç†æµæ°´çº¿æ„å»º**
- **RAG ç³»ç»Ÿä¸­çš„å›¾è¡¨å¤„ç†æ–¹æ¡ˆ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/document_processing)

</details>

### åµŒå…¥æ¨¡å‹

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å‘é‡åµŒå…¥ä¸åµŒå…¥æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µ**
- **LLM åº”ç”¨ä¸­åµŒå…¥æ¨¡å‹çš„ä½¿ç”¨åœºæ™¯**
- **é•¿çŸ­å†…å®¹åµŒå…¥çš„åŒºåˆ«ä¸ä¼˜åŒ–**
- **å¦‚ä½•åŸºäºç§æœ‰æ•°æ®è¯„æµ‹åµŒå…¥æ¨¡å‹**
- **OpenAI åµŒå…¥æ¨¡å‹ç²¾åº¦ä¸è¶³çš„ä¼˜åŒ–æ–¹æ¡ˆ**
- **æ”¹è¿› Sentence Transformer æ¨¡å‹çš„æ­¥éª¤**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/embedding_models)

</details>

### å‘é‡æ•°æ®åº“åŸç†

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å‘é‡æ•°æ®åº“çš„åŸºæœ¬åŸç†**
- **å‘é‡æ•°æ®åº“ä¸ä¼ ç»Ÿæ•°æ®åº“çš„å·®å¼‚**
- **ç´¢å¼•ã€æ•°æ®åº“ä¸æ’ä»¶çš„åŒºåˆ«**
- **é«˜ç²¾åº¦æœç´¢åœºæ™¯ä¸‹çš„ç­–ç•¥é€‰æ‹©**
- **èšç±»ä¸å±€éƒ¨æ•æ„Ÿå“ˆå¸Œç­‰æœç´¢ç­–ç•¥**
- **èšç±»å‡å°‘æœç´¢ç©ºé—´çš„æœºåˆ¶**
- **éšæœºæŠ•å½±ç´¢å¼•å·¥ä½œåŸç†**
- **å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ(LSH)å®ç°æœºåˆ¶**
- **ä¹˜ç§¯é‡åŒ–(PQ)ç´¢å¼•æ–¹æ³•**
- **ä¸åŒå‘é‡ç´¢å¼•çš„åœºæ™¯åº”ç”¨æ¯”è¾ƒ**
- **ç›¸ä¼¼åº¦åº¦é‡çš„é€‰æ‹©æ ‡å‡†**
- **å‘é‡æ•°æ®åº“è¿‡æ»¤çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ**
- **å‘é‡æ•°æ®åº“é€‰å‹æŒ‡å—**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/vector_databases)

</details>

### é«˜çº§æœç´¢ç®—æ³•

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **ä¿¡æ¯æ£€ç´¢ä¸è¯­ä¹‰æœç´¢çš„æ¶æ„æ¨¡å¼**
- **é«˜è´¨é‡æœç´¢ç³»ç»Ÿçš„é‡è¦æ€§**
- **å¤§è§„æ¨¡æ•°æ®é›†çš„é«˜æ•ˆç²¾å‡†æœç´¢**
- **æ”¹è¿›ä¸å‡†ç¡® RAG æ£€ç´¢ç³»ç»Ÿçš„æ­¥éª¤**
- **åŸºäºå…³é”®è¯çš„æ£€ç´¢æ–¹æ³•**
- **ä¼˜åŒ–é‡æ’æ¨¡å‹çš„å¾®è°ƒæŠ€æœ¯**
- **ä¿¡æ¯æ£€ç´¢å¸¸ç”¨æŒ‡æ ‡åŠå±€é™æ€§**
- **ç±» Quora ç³»ç»Ÿçš„è¯„ä»·æŒ‡æ ‡é€‰æ‹©**
- **æ¨èç³»ç»Ÿçš„è¯„ä»·æŒ‡æ ‡**
- **ä¸åŒä¿¡æ¯æ£€ç´¢æŒ‡æ ‡çš„åº”ç”¨åœºæ™¯**
- **æ··åˆæœç´¢çš„å·¥ä½œåŸç†**
- **å¤šæºæœç´¢ç»“æœçš„åˆå¹¶ç­–ç•¥**
- **å¤šè½®æŸ¥è¯¢çš„å¤„ç†æŠ€æœ¯**
- **æ”¹è¿›æ£€ç´¢æ•ˆæœçš„é«˜çº§æŠ€æœ¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/search_algorithms)

</details>

### è¯­è¨€æ¨¡å‹åŸç†

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è¯¦ç»†è§£æ**
- **è‡ªæ³¨æ„åŠ›çš„ç¼ºé™·ä¸æ”¹è¿›æ–¹æ¡ˆ**
- **ä½ç½®ç¼–ç çš„å·¥ä½œåŸç†**
- **Transformer æ¶æ„æ·±åº¦å‰–æ**
- **Transformer ç›¸å¯¹ LSTM çš„ä¼˜åŠ¿**
- **å±€éƒ¨æ³¨æ„åŠ›å’Œå…¨å±€æ³¨æ„åŠ›çš„åŒºåˆ«**
- **Transformer è®¡ç®—èµ„æºæ¶ˆè€—ä¼˜åŒ–**
- **æ‰©å±• LLM ä¸Šä¸‹æ–‡é•¿åº¦çš„æŠ€æœ¯**
- **å¤§è¯è¡¨ä¸‹çš„æ¶æ„ä¼˜åŒ–æ–¹æ¡ˆ**
- **è¯è¡¨å¤§å°å¹³è¡¡ç­–ç•¥**
- **ä¸åŒ LLM æ¶æ„çš„é€‚ç”¨åœºæ™¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/llm_internals)

</details>

### ç›‘ç£å¾®è°ƒ (SFT)

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å¾®è°ƒçš„æ¦‚å¿µä¸å¿…è¦æ€§**
- **éœ€è¦å¾®è°ƒçš„åœºæ™¯åˆ†æ**
- **å¾®è°ƒå†³ç­–çš„è¯„ä¼°æµç¨‹**
- **åŸºäºä¸Šä¸‹æ–‡çš„ç²¾ç¡®å›ç­”ä¼˜åŒ–**
- **QA å¾®è°ƒæ•°æ®é›†æ„å»ºæ–¹æ³•**
- **å¾®è°ƒè¶…å‚æ•°è®¾ç½®æŒ‡å—**
- **å¾®è°ƒåŸºç¡€è®¾æ–½éœ€æ±‚ä¼°ç®—**
- **æ¶ˆè´¹çº§ç¡¬ä»¶çš„å¾®è°ƒæ–¹æ¡ˆ**
- **å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•åˆ†ç±»**
- **ç¾éš¾æ€§é—å¿˜é—®é¢˜è§£æ**
- **é‡å‚æ•°åŒ–å¾®è°ƒæ–¹æ³•**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/fine_tuning)

</details>

### åå¥½å¯¹é½ (RLHF/DPO)

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **é€‰æ‹©åå¥½å¯¹é½æ–¹æ³•çš„æ—¶æœº**
- **RLHF çš„å·¥ä½œæœºåˆ¶ä¸åº”ç”¨**
- **RLHF ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜**
- **ä¸åŒåå¥½å¯¹é½æ–¹æ³•æ¯”è¾ƒ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/preference_alignment)

</details>

### è¯„ä¼° LLM ç³»ç»Ÿ

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å¦‚ä½•è¯„ä¼°æœ€é€‚åˆçš„ LLM æ¨¡å‹**
- **RAG ç³»ç»Ÿè¯„ä¼°æ–¹æ³•è®º**
- **LLM è¯„ä¼°æŒ‡æ ‡å¤§å…¨**
- **éªŒè¯é“¾(Chain of Verification)è§£æ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/llm_evaluation)

</details>

### å¹»è§‰æ§åˆ¶æŠ€æœ¯

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **ä¸åŒå½¢å¼çš„å¹»è§‰åˆ†ç±»**
- **å¤šå±‚æ¬¡å¹»è§‰æ§åˆ¶æŠ€æœ¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/hallucination_control)

</details>

### LLM éƒ¨ç½²æ–¹æ¡ˆ

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **é‡åŒ–ä¸å½±å“ç²¾åº¦çš„åŸç†**
- **LLM æ¨ç†ååé‡ä¼˜åŒ–æŠ€æœ¯**
- **æ— æ³¨æ„åŠ›è¿‘ä¼¼çš„å“åº”åŠ é€Ÿæ–¹æ¡ˆ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/llm_deployment)

</details>

### æ™ºèƒ½ä½“ç³»ç»Ÿ

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **æ™ºèƒ½ä½“åŸºæœ¬æ¦‚å¿µä¸å®ç°ç­–ç•¥**
- **æ™ºèƒ½ä½“çš„éœ€æ±‚ä¸å¸¸è§æ¶æ„**
- **ReAct æç¤ºå®ç°ç¤ºä¾‹**
- **è®¡åˆ’ä¸æ‰§è¡Œç­–ç•¥è¯¦è§£**
- **OpenAI å‡½æ•°ä½¿ç”¨å®ä¾‹**
- **OpenAI å‡½æ•° vs LangChain æ™ºèƒ½ä½“**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/agent_systems)

</details>

### æç¤ºæ³¨å…¥æ”»é˜²

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **æç¤ºæ”»å‡»çš„åŸºæœ¬æ¦‚å¿µä¸å±å®³**
- **ä¸åŒç±»å‹æç¤ºæ”»å‡»åˆ†æ**
- **é˜²å¾¡æç¤ºæ”»å‡»çš„ç­–ç•¥**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/prompt_hacking)

</details>

### ç»¼åˆè¯é¢˜

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **LLM ç³»ç»Ÿæˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ**
- **ä¸“å®¶æ··åˆæ¨¡å‹(MoE)è§£æ**
- **ç”Ÿäº§çº§ RAG ç³»ç»Ÿæ„å»ºæŒ‡å—**
- **FP8 å˜é‡åŠå…¶ä¼˜åŠ¿**
- **æ— æŸç²¾åº¦ä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯**
- **KV ç¼“å­˜å¤§å°è®¡ç®—æ–¹æ³•**
- **å¤šå¤´æ³¨æ„åŠ›å±‚ç»´åº¦åˆ†æ**
- **æ³¨æ„åŠ›å±‚ç„¦ç‚¹æ§åˆ¶æŠ€æœ¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/miscellaneous)

</details>

### å®æˆ˜æ¡ˆä¾‹

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>
æ•¬è¯·æœŸå¾…
</details>

### ğŸ¤ å¦‚ä½•è´¡çŒ®

1. æäº¤æ–°é—®é¢˜åˆ°å¯¹åº”åˆ†ç±»çš„ `.md` æ–‡ä»¶
2. å®Œå–„ç°æœ‰é—®é¢˜ç­”æ¡ˆï¼ˆéœ€æ ‡æ³¨å¼•ç”¨æ¥æºï¼‰
3. æ”¹è¿›æ–‡æ¡£ç»“æ„æˆ–ç¿»è¯‘
4. æ¬¢è¿æäº¤çœŸå®é¢è¯•ç»å†ï¼

</div>
<div style="grid-column: 2;">
[è¿”å›é¡¶éƒ¨â†‘](#section-zh)


<a name="section-en"></a>

<!-- è‹±æ–‡éƒ¨åˆ† -->

# LLM Interview All You Need

ğŸ“š Curating top company LLM interview questions with community collaboration!
ğŸš€ Continuously updating real questions from Google/OpenAI/Meta/Anthropic covering RAG, fine-tuning, deployment and more.

## Table of Contents

1. [Prompt Engineering & LLM Basics](#prompt-engineering--basics-of-llm)
2. [Retrieval Augmented Generation (RAG)](#retrieval-augmented-generation-rag)
3. [Document Digitization & Chunking](#document-digitization--chunking)
4. [Embedding Models](#embedding-models)
5. [Vector DB Internals](#internal-working-of-vector-databases)
6. [Advanced Search Algorithms](#advanced-search-algorithms)
7. [Language Model Internals](#language-models-internal-working)
8. [Supervised Fine-Tuning (SFT)](#supervised-fine-tuning-of-llm)
9. [Preference Alignment (RLHF/DPO)](#preference-alignment-rlhfdpo)
10. [Evaluating LLM Systems](#evaluation-of-llm-system)
11. [Hallucination Control](#hallucination-control-techniques)
12. [LLM Deployment Strategies](#deployment-of-llm)
13. [Agent-Based Systems](#agent-based-system)
14. [Prompt Hacking Defenses](#prompt-hacking)
15. [Miscellaneous Topics](#miscellaneous)
16. [Case Studies](#case-studies)

---

### Prompt Engineering & Basics

<details>
<summary>View Questions</summary>

- **What is the difference between Predictive/Discriminative AI and Generative AI?**
- **What is LLM, and how are LLMs trained?**
- **Explain the Temperature parameter and how to set it**
- **What are different decoding strategies for tokens?**
- **Stopping criteria definition methods**
- **How to use stop sequences in LLMs?**
- **Basic structure of prompt engineering**
- **Explain in-context learning**
- **Types of prompt engineering**
- **Aspects to keep in mind with few-shots prompting**
- **Strategies to write effective prompts**
- **Hallucination control via prompts**
- **Improving reasoning through prompts**
- **Solution when COT prompt fails**

[View Full Document â†’](/docs/prompt_engineering_basics)

</details>

### Retrieval Augmented Generation

<details>
<summary>View Questions</summary>

- **Increase accuracy & reliability in LLM**
- **How RAG works?**
- **Benefits of RAG systems**
- **Fine-tuning vs RAG selection**
- **LLM customization patterns**

[View Full Document â†’](/docs/rag_systems)

</details>

### Document Digitization & Chunking

<details>
<summary>View Questions</summary>

- **What is chunking and why needed**
- **Factors influencing chunk size**
- **Different chunking methods**
- **Finding ideal chunk size**
- **Digitizing complex documents**
- **Handling tables during chunking**
- **Retrieval optimization for large tables**
- **List item processing techniques**
- **Production document pipelines**
- **Handling graphs & charts in RAG**

[View Full Document â†’](/docs/document_processing)

</details>

### Embedding Models

<details>
<summary>View Questions</summary>

- **Vector embeddings basics**
- **Embedding model usage in LLM apps**
- **Short vs long content embedding**
- **Benchmarking embedding models**
- **Improving low-accuracy embedding model**
- **Enhancing sentence transformers**

[View Full Document â†’](/docs/embedding_models)

</details>

### Vector DB Internals

<details>
<summary>View Questions</summary>

- **What is a vector database?**
- **How does a vector database differ from traditional databases?**
- **How does a vector database work?**
- **Explain difference between vector index, vector DB & vector plugins?**
- **You are working on a project that involves a small dataset of customer reviews. Your task is to find similar reviews in the dataset. The priority is to achieve perfect accuracy in finding the most similar reviews, and the speed of the search is not a primary concern. Which search strategy would you choose and why?**
- **Explain vector search strategies like clustering and Locality-Sensitive Hashing.**
- **How does clustering reduce search space? When does it fail and how can we mitigate these failures?**
- **Explain Random projection index?**
- **Explain Locality-sensitive hashing (LHS) indexing method?**
- **Explain product quantization (PQ) indexing method?**
- **Compare different Vector index and given a scenario, which vector index you would use for a project?**
- **How would you decide ideal search similarity metrics for the use case?**
- **Explain different types and challenges associated with filtering in vector DB?**
- **How to decide the best vector database for your needs?**

[View Full Document â†’](/docs/vector_databases)

</details>

### Advanced Search Algorithms

<details>
<summary>View Questions</summary>

- **What are architecture patterns for information retrieval & semantic search?**
- **Why itâ€™s important to have very good search**
- **How can you achieve efficient and accurate search results in large-scale datasets?**
- **Consider a scenario where a client has already built a RAG-based system that is not giving accurate results, upon investigation you find out that the retrieval system is not accurate, what steps you will take to improve it?**
- **Explain the keyword-based retrieval method**
- **How to fine-tune re-ranking models?**
- **Explain most common metric used in information retrieval and when it fails?**
- **If you were to create an algorithm for a Quora-like question-answering system, with the objective of ensuring users find the most pertinent answers as quickly as possible, which evaluation metric would you choose to assess the effectiveness of your system?**
- **I have a recommendation system, which metric should I use to evaluate the system?**
- **Compare different information retrieval metrics and which one to use when?**
- **How does hybrid search works?**
- **If you have search results from multiple methods, how would you merge and homogenize the rankings into a single result set?**
- **How to handle multi-hop/multifaceted queries?**
- **What are different techniques to be used to improved retrieval?**

[View Full Document â†’](/docs/search_algorithms)

</details>

### LLM Internals

<details>
<summary>View Questions</summary>

- **Can you provide a detailed explanation of the concept of self-attention?**
- **Explain the disadvantages of the self-attention mechanism and how can you overcome it.**
- **What is positional encoding?**
- **Explain Transformer architecture in detail.**
- **What are some of the advantages of using a transformer instead of LSTM?**
- **What is the difference between local attention and global attention?**
- **What makes transformers heavy on computation and memory, and how can we address this?**
- **How can you increase the context length of an LLM?**
- **If I have a vocabulary of 100K words/tokens, how can I optimize transformer architecture?**
- **A large vocabulary can cause computation issues and a small vocabulary can cause OOV issues, what approach you would use to find the best balance of vocabulary?**
- **Explain different types of LLM architecture and which type of architecture is best for which task?**

[View Full Document â†’](/docs/llm_internals)

</details>

### Supervised Fine-Tuning

<details>
<summary>View Questions</summary>

- **What is fine-tuning, and why is it needed?**
- **Which scenario do we need to fine-tune LLM?**
- **How to make the decision of fine-tuning?**
- **How do you improve the model to answer only if there is sufficient context for doing so?**
- **How to create fine-tuning datasets for Q&A?**
- **How to set hyperparameters for fine-tuning?**
- **How to estimate infrastructure requirements for fine-tuning LLM?**
- **How do you fine-tune LLM on consumer hardware?**
- **What are the different categories of the PEFT method?**
- **What is catastrophic forgetting in LLMs?**
- **What are different re-parameterized methods for fine-tuning?**

[View Full Document â†’](/docs/fine_tuning)

</details>

### Preference Alignment

<details>
<summary>View Questions</summary>

- **At which stage you will decide to go for the Preference alignment type of method rather than SFT?**
- **What is RLHF, and how is it used?**
- **What is the reward hacking issue in RLHF?**
- **Explain different preference alignment methods.**

[View Full Document â†’](/docs/preference_alignment)

</details>

### Evaluating LLM Systems

<details>
<summary>View Questions</summary>

- **How do you evaluate the best LLM model for your use case?**
- **How to evaluate RAG-based systems?**
- **What are different metrics for evaluating LLMs?**
- **Explain the Chain of Verification.**

[View Full Document â†’](/docs/llm_evaluation)

</details>

### Hallucination Control

<details>
<summary>View Questions</summary>

- **What are different forms of hallucinations?**
- **How to control hallucinations at various levels?**

[View Full Document â†’](/docs/hallucination_control)

</details>

### LLM Deployment

<details>
<summary>View Questions</summary>

- **Why quantization preserves accuracy**
- **LLM inference throughput optimization**
- **Accelerating response time**

[View Full Document â†’](/docs/llm_deployment)

</details>

### Agent-Based Systems

<details>
<summary>View Questions</summary>

- **Agent concepts & strategies**
- **Need for agents & implementation**
- **ReAct prompting example**
- **Plan and Execute strategy**
- **OpenAI functions examples**
- **OpenAI vs LangChain Agents**

[View Full Document â†’](/docs/agent_systems)

</details>

### Prompt Hacking

<details>
<summary>View Questions</summary>

- **Prompt hacking explained**
- **Types of prompt hacking**
- **Defense tactics**

[View Full Document â†’](/docs/prompt_hacking)

</details>

### Miscellaneous

<details>
<summary>View Questions</summary>

- **Optimizing LLM system cost**
- **Mixture of Expert (MoE) models**
- **Building production RAG system**
- **FP8 advantages**
- **Low precision training**
- **KV cache size calculation**
- **Attention layer dimensions**
- **Focusing attention layers**

[View Full Document â†’](/docs/miscellaneous)

</details>

### Case Studies

<details>
<summary>View Questions</summary>

Coming soon

</details>

### ğŸ¤ How to Contribute

1. Add new questions to corresponding `.md` files
2. Improve existing answers (with citations)
3. Enhance documentation structure or translations
4. Share real interview experiences!

</div>
</div>
[Back to Topâ†‘](#section-en)



[Back to Topâ†‘](#section-en)## æˆæƒåè®® (License)

ç‰ˆæƒæ‰€æœ‰ (c) 2025 llm_interview_all_you_need

ç‰¹æ­¤æˆäºˆä»»ä½•è·å¾—æœ¬è½¯ä»¶å‰¯æœ¬åŠç›¸å…³æ–‡æ¡£æ–‡ä»¶ï¼ˆä»¥ä¸‹ç®€ç§°"è½¯ä»¶"ï¼‰çš„ä»»ä½•äººå…è´¹è®¸å¯ï¼Œå…è®¸å…¶ä¸å—é™åˆ¶åœ°å¤„ç†æœ¬è½¯ä»¶ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹ã€åˆå¹¶ã€å‘å¸ƒã€åˆ†å‘ã€å†æˆæƒåŠ/æˆ–é”€å”®è½¯ä»¶å‰¯æœ¬çš„æƒåˆ©ï¼Œå¹¶å…è®¸è·å¾—è½¯ä»¶çš„äººè¿™æ ·åšï¼Œä½†é¡»æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

ä¸Šè¿°ç‰ˆæƒå£°æ˜å’Œæœ¬è®¸å¯å£°æ˜åº”åŒ…å«åœ¨è½¯ä»¶çš„æ‰€æœ‰å‰¯æœ¬æˆ–å®è´¨æ€§éƒ¨åˆ†ä¸­ã€‚

æœ¬è½¯ä»¶æŒ‰"åŸæ ·"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºé€‚é”€æ€§ä¿è¯ã€ç‰¹å®šç”¨é€”é€‚ç”¨æ€§ä¿è¯å’Œéä¾µæƒä¿è¯ã€‚åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œä½œè€…æˆ–ç‰ˆæƒæ‰€æœ‰è€…å‡ä¸å¯¹å› è½¯ä»¶æˆ–è½¯ä»¶ä½¿ç”¨æˆ–å…¶ä»–äº¤æ˜“è¡Œä¸ºè€Œäº§ç”Ÿçš„ä»»ä½•ç´¢èµ”ã€æŸå®³æˆ–å…¶ä»–è´£ä»»è´Ÿè´£ã€‚

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px;">
<div style="grid-column: 1;">

<!-- ä¸­æ–‡éƒ¨åˆ† -->

# LLM é¢è¯•å®Œå…¨æŒ‡å— (llm_interview_all_you_need)

ğŸ“š æ”¶é›†é¡¶çº§å…¬å¸ LLM é¢è¯•é«˜é¢‘é—®é¢˜ï¼Œæ¬¢è¿ç¤¾åŒºå…±åˆ›ï¼
ğŸš€ æ­£åœ¨ä¸æ–­æ•´ç†æ¥è‡ª Google/OpenAI/Meta/Anthropic ç­‰å…¬å¸çš„çœŸé¢˜ï¼Œæ¶µç›– RAGã€å¾®è°ƒã€éƒ¨ç½²ç­‰æ ¸å¿ƒé¢†åŸŸã€‚

## ç›®å½•

1. [æç¤ºå·¥ç¨‹ä¸ LLM åŸºç¡€](#prompt-engineering--basics-of-llm)
2. [æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)](#retrieval-augmented-generation-rag)
3. [æ–‡æ¡£æ•°å­—åŒ–ä¸åˆ†å—](#document-digitization--chunking)
4. [åµŒå…¥æ¨¡å‹](#embedding-models)
5. [å‘é‡æ•°æ®åº“åŸç†](#internal-working-of-vector-databases)
6. [é«˜çº§æœç´¢ç®—æ³•](#advanced-search-algorithms)
7. [è¯­è¨€æ¨¡å‹åŸç†](#language-models-internal-working)
8. [ç›‘ç£å¾®è°ƒ (SFT)](#supervised-fine-tuning-of-llm)
9. [åå¥½å¯¹é½ (RLHF/DPO)](#preference-alignment-rlhfdpo)
10. [è¯„ä¼° LLM ç³»ç»Ÿ](#evaluation-of-llm-system)
11. [å¹»è§‰æ§åˆ¶æŠ€æœ¯](#hallucination-control-techniques)
12. [LLM éƒ¨ç½²æ–¹æ¡ˆ](#deployment-of-llm)
13. [æ™ºèƒ½ä½“ç³»ç»Ÿ](#agent-based-system)
14. [æç¤ºæ³¨å…¥æ”»é˜²](#prompt-hacking)
15. [ç»¼åˆè¯é¢˜](#miscellaneous)
16. [å®æˆ˜æ¡ˆä¾‹](#case-studies)

---

### æç¤ºå·¥ç¨‹ä¸ LLM åŸºç¡€

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **ç”Ÿæˆå¼ AI ä¸åˆ¤åˆ«å¼ AI çš„æ ¸å¿ƒåŒºåˆ«ï¼Ÿ**
- **è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæµç¨‹è§£æ**
- **Temperature å‚æ•°çš„ä½œç”¨ä¸è®¾ç½®åŸåˆ™**
- **LLM è§£ç ç­–ç•¥æ¯”è¾ƒåˆ†æ**
- **å¦‚ä½•å®šä¹‰å¤§è¯­è¨€æ¨¡å‹çš„åœæ­¢æ¡ä»¶ï¼Ÿ**
- **åœæ­¢åºåˆ—åœ¨ LLM ä¸­çš„åº”ç”¨æ–¹æ³•**
- **æç¤ºå·¥ç¨‹çš„åŸºæœ¬ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿ**
- **ä¸Šä¸‹æ–‡å­¦ä¹ æœºåˆ¶è§£æ**
- **æç¤ºå·¥ç¨‹çš„ç±»å‹ä¸å®æ–½æ–¹æ³•**
- **å°‘æ ·æœ¬æç¤ºçš„å…³é”®æ³¨æ„äº‹é¡¹**
- **ç¼–å†™é«˜è´¨é‡æç¤ºçš„æœ‰æ•ˆç­–ç•¥**
- **å¦‚ä½•é€šè¿‡æç¤ºå·¥ç¨‹æ§åˆ¶ LLM å¹»è§‰**
- **ä½¿ç”¨æç¤ºå·¥ç¨‹å¢å¼º LLM æ¨ç†èƒ½åŠ›**
- **å½“æ€ç»´é“¾(CoT)æç¤ºå¤±æ•ˆæ—¶çš„æ”¹è¿›æ–¹æ³•**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/prompt_engineering_basics)

</details>

### æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å¦‚ä½•æé«˜ LLM è¾“å‡ºçš„å‡†ç¡®æ€§å’Œå¯é æ€§**
- **RAG å·¥ä½œæœºåˆ¶è¯¦ç»†è§£æ**
- **ä½¿ç”¨ RAG ç³»ç»Ÿçš„ä¸»è¦ä¼˜åŠ¿**
- **å¾®è°ƒ vs RAG çš„é€‰æ‹©æ ‡å‡†**
- **ç§æœ‰æ•°æ®å®šåˆ¶åŒ– LLM çš„æ¶æ„æ¨¡å¼**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/rag_systems)

</details>

### æ–‡æ¡£æ•°å­—åŒ–ä¸åˆ†å—

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **åˆ†å—çš„åŸºæœ¬æ¦‚å¿µä¸å¿…è¦æ€§**
- **å½±å“åˆ†å—å¤§å°çš„å…³é”®å› ç´ **
- **ä¸åŒç±»å‹çš„åˆ†å—æ–¹æ³•æ¯”è¾ƒ**
- **å¯»æ‰¾æœ€ä½³åˆ†å—å¤§å°çš„ç­–ç•¥**
- **å¤æ‚æ–‡æ¡£ï¼ˆå¹´æŠ¥ï¼‰çš„æ•°å­—åŒ–å¤„ç†æ–¹æ¡ˆ**
- **è¡¨æ ¼å¤„ç†çš„æœ€ä½³å®è·µ**
- **å¤§å‹è¡¨æ ¼çš„æ£€ç´¢ä¼˜åŒ–æ–¹æ³•**
- **åˆ—è¡¨é¡¹çš„åˆ†å—å¤„ç†æŠ€æœ¯**
- **ç”Ÿäº§çº§æ–‡æ¡£å¤„ç†æµæ°´çº¿æ„å»º**
- **RAG ç³»ç»Ÿä¸­çš„å›¾è¡¨å¤„ç†æ–¹æ¡ˆ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/document_processing)

</details>

### åµŒå…¥æ¨¡å‹

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å‘é‡åµŒå…¥ä¸åµŒå…¥æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µ**
- **LLM åº”ç”¨ä¸­åµŒå…¥æ¨¡å‹çš„ä½¿ç”¨åœºæ™¯**
- **é•¿çŸ­å†…å®¹åµŒå…¥çš„åŒºåˆ«ä¸ä¼˜åŒ–**
- **å¦‚ä½•åŸºäºç§æœ‰æ•°æ®è¯„æµ‹åµŒå…¥æ¨¡å‹**
- **OpenAI åµŒå…¥æ¨¡å‹ç²¾åº¦ä¸è¶³çš„ä¼˜åŒ–æ–¹æ¡ˆ**
- **æ”¹è¿› Sentence Transformer æ¨¡å‹çš„æ­¥éª¤**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/embedding_models)

</details>

### å‘é‡æ•°æ®åº“åŸç†

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å‘é‡æ•°æ®åº“çš„åŸºæœ¬åŸç†**
- **å‘é‡æ•°æ®åº“ä¸ä¼ ç»Ÿæ•°æ®åº“çš„å·®å¼‚**
- **ç´¢å¼•ã€æ•°æ®åº“ä¸æ’ä»¶çš„åŒºåˆ«**
- **é«˜ç²¾åº¦æœç´¢åœºæ™¯ä¸‹çš„ç­–ç•¥é€‰æ‹©**
- **èšç±»ä¸å±€éƒ¨æ•æ„Ÿå“ˆå¸Œç­‰æœç´¢ç­–ç•¥**
- **èšç±»å‡å°‘æœç´¢ç©ºé—´çš„æœºåˆ¶**
- **éšæœºæŠ•å½±ç´¢å¼•å·¥ä½œåŸç†**
- **å±€éƒ¨æ•æ„Ÿå“ˆå¸Œ(LSH)å®ç°æœºåˆ¶**
- **ä¹˜ç§¯é‡åŒ–(PQ)ç´¢å¼•æ–¹æ³•**
- **ä¸åŒå‘é‡ç´¢å¼•çš„åœºæ™¯åº”ç”¨æ¯”è¾ƒ**
- **ç›¸ä¼¼åº¦åº¦é‡çš„é€‰æ‹©æ ‡å‡†**
- **å‘é‡æ•°æ®åº“è¿‡æ»¤çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ**
- **å‘é‡æ•°æ®åº“é€‰å‹æŒ‡å—**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/vector_databases)

</details>

### é«˜çº§æœç´¢ç®—æ³•

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **ä¿¡æ¯æ£€ç´¢ä¸è¯­ä¹‰æœç´¢çš„æ¶æ„æ¨¡å¼**
- **é«˜è´¨é‡æœç´¢ç³»ç»Ÿçš„é‡è¦æ€§**
- **å¤§è§„æ¨¡æ•°æ®é›†çš„é«˜æ•ˆç²¾å‡†æœç´¢**
- **æ”¹è¿›ä¸å‡†ç¡® RAG æ£€ç´¢ç³»ç»Ÿçš„æ­¥éª¤**
- **åŸºäºå…³é”®è¯çš„æ£€ç´¢æ–¹æ³•**
- **ä¼˜åŒ–é‡æ’æ¨¡å‹çš„å¾®è°ƒæŠ€æœ¯**
- **ä¿¡æ¯æ£€ç´¢å¸¸ç”¨æŒ‡æ ‡åŠå±€é™æ€§**
- **ç±» Quora ç³»ç»Ÿçš„è¯„ä»·æŒ‡æ ‡é€‰æ‹©**
- **æ¨èç³»ç»Ÿçš„è¯„ä»·æŒ‡æ ‡**
- **ä¸åŒä¿¡æ¯æ£€ç´¢æŒ‡æ ‡çš„åº”ç”¨åœºæ™¯**
- **æ··åˆæœç´¢çš„å·¥ä½œåŸç†**
- **å¤šæºæœç´¢ç»“æœçš„åˆå¹¶ç­–ç•¥**
- **å¤šè½®æŸ¥è¯¢çš„å¤„ç†æŠ€æœ¯**
- **æ”¹è¿›æ£€ç´¢æ•ˆæœçš„é«˜çº§æŠ€æœ¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/search_algorithms)

</details>

### è¯­è¨€æ¨¡å‹åŸç†

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è¯¦ç»†è§£æ**
- **è‡ªæ³¨æ„åŠ›çš„ç¼ºé™·ä¸æ”¹è¿›æ–¹æ¡ˆ**
- **ä½ç½®ç¼–ç çš„å·¥ä½œåŸç†**
- **Transformer æ¶æ„æ·±åº¦å‰–æ**
- **Transformer ç›¸å¯¹ LSTM çš„ä¼˜åŠ¿**
- **å±€éƒ¨æ³¨æ„åŠ›å’Œå…¨å±€æ³¨æ„åŠ›çš„åŒºåˆ«**
- **Transformer è®¡ç®—èµ„æºæ¶ˆè€—ä¼˜åŒ–**
- **æ‰©å±• LLM ä¸Šä¸‹æ–‡é•¿åº¦çš„æŠ€æœ¯**
- **å¤§è¯è¡¨ä¸‹çš„æ¶æ„ä¼˜åŒ–æ–¹æ¡ˆ**
- **è¯è¡¨å¤§å°å¹³è¡¡ç­–ç•¥**
- **ä¸åŒ LLM æ¶æ„çš„é€‚ç”¨åœºæ™¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/llm_internals)

</details>

### ç›‘ç£å¾®è°ƒ (SFT)

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å¾®è°ƒçš„æ¦‚å¿µä¸å¿…è¦æ€§**
- **éœ€è¦å¾®è°ƒçš„åœºæ™¯åˆ†æ**
- **å¾®è°ƒå†³ç­–çš„è¯„ä¼°æµç¨‹**
- **åŸºäºä¸Šä¸‹æ–‡çš„ç²¾ç¡®å›ç­”ä¼˜åŒ–**
- **QA å¾®è°ƒæ•°æ®é›†æ„å»ºæ–¹æ³•**
- **å¾®è°ƒè¶…å‚æ•°è®¾ç½®æŒ‡å—**
- **å¾®è°ƒåŸºç¡€è®¾æ–½éœ€æ±‚ä¼°ç®—**
- **æ¶ˆè´¹çº§ç¡¬ä»¶çš„å¾®è°ƒæ–¹æ¡ˆ**
- **å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æ–¹æ³•åˆ†ç±»**
- **ç¾éš¾æ€§é—å¿˜é—®é¢˜è§£æ**
- **é‡å‚æ•°åŒ–å¾®è°ƒæ–¹æ³•**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/fine_tuning)

</details>

### åå¥½å¯¹é½ (RLHF/DPO)

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **é€‰æ‹©åå¥½å¯¹é½æ–¹æ³•çš„æ—¶æœº**
- **RLHF çš„å·¥ä½œæœºåˆ¶ä¸åº”ç”¨**
- **RLHF ä¸­çš„å¥–åŠ±é»‘å®¢é—®é¢˜**
- **ä¸åŒåå¥½å¯¹é½æ–¹æ³•æ¯”è¾ƒ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/preference_alignment)

</details>

### è¯„ä¼° LLM ç³»ç»Ÿ

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **å¦‚ä½•è¯„ä¼°æœ€é€‚åˆçš„ LLM æ¨¡å‹**
- **RAG ç³»ç»Ÿè¯„ä¼°æ–¹æ³•è®º**
- **LLM è¯„ä¼°æŒ‡æ ‡å¤§å…¨**
- **éªŒè¯é“¾(Chain of Verification)è§£æ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/llm_evaluation)

</details>

### å¹»è§‰æ§åˆ¶æŠ€æœ¯

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **ä¸åŒå½¢å¼çš„å¹»è§‰åˆ†ç±»**
- **å¤šå±‚æ¬¡å¹»è§‰æ§åˆ¶æŠ€æœ¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/hallucination_control)

</details>

### LLM éƒ¨ç½²æ–¹æ¡ˆ

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **é‡åŒ–ä¸å½±å“ç²¾åº¦çš„åŸç†**
- **LLM æ¨ç†ååé‡ä¼˜åŒ–æŠ€æœ¯**
- **æ— æ³¨æ„åŠ›è¿‘ä¼¼çš„å“åº”åŠ é€Ÿæ–¹æ¡ˆ**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/llm_deployment)

</details>

### æ™ºèƒ½ä½“ç³»ç»Ÿ

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **æ™ºèƒ½ä½“åŸºæœ¬æ¦‚å¿µä¸å®ç°ç­–ç•¥**
- **æ™ºèƒ½ä½“çš„éœ€æ±‚ä¸å¸¸è§æ¶æ„**
- **ReAct æç¤ºå®ç°ç¤ºä¾‹**
- **è®¡åˆ’ä¸æ‰§è¡Œç­–ç•¥è¯¦è§£**
- **OpenAI å‡½æ•°ä½¿ç”¨å®ä¾‹**
- **OpenAI å‡½æ•° vs LangChain æ™ºèƒ½ä½“**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/agent_systems)

</details>

### æç¤ºæ³¨å…¥æ”»é˜²

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **æç¤ºæ”»å‡»çš„åŸºæœ¬æ¦‚å¿µä¸å±å®³**
- **ä¸åŒç±»å‹æç¤ºæ”»å‡»åˆ†æ**
- **é˜²å¾¡æç¤ºæ”»å‡»çš„ç­–ç•¥**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/prompt_hacking)

</details>

### ç»¼åˆè¯é¢˜

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>

- **LLM ç³»ç»Ÿæˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ**
- **ä¸“å®¶æ··åˆæ¨¡å‹(MoE)è§£æ**
- **ç”Ÿäº§çº§ RAG ç³»ç»Ÿæ„å»ºæŒ‡å—**
- **FP8 å˜é‡åŠå…¶ä¼˜åŠ¿**
- **æ— æŸç²¾åº¦ä½ç²¾åº¦è®­ç»ƒæŠ€æœ¯**
- **KV ç¼“å­˜å¤§å°è®¡ç®—æ–¹æ³•**
- **å¤šå¤´æ³¨æ„åŠ›å±‚ç»´åº¦åˆ†æ**
- **æ³¨æ„åŠ›å±‚ç„¦ç‚¹æ§åˆ¶æŠ€æœ¯**

[æŸ¥çœ‹å®Œæ•´æ–‡æ¡£ â†’](/docs/miscellaneous)

</details>

### å®æˆ˜æ¡ˆä¾‹

<details>
<summary>æŸ¥çœ‹é—®é¢˜</summary>
æ•¬è¯·æœŸå¾…
</details>

### ğŸ¤ å¦‚ä½•è´¡çŒ®

1. æäº¤æ–°é—®é¢˜åˆ°å¯¹åº”åˆ†ç±»çš„ `.md` æ–‡ä»¶
2. å®Œå–„ç°æœ‰é—®é¢˜ç­”æ¡ˆï¼ˆéœ€æ ‡æ³¨å¼•ç”¨æ¥æºï¼‰
3. æ”¹è¿›æ–‡æ¡£ç»“æ„æˆ–ç¿»è¯‘
4. æ¬¢è¿æäº¤çœŸå®é¢è¯•ç»å†ï¼

</div>
<div style="grid-column: 2;">


<!-- è‹±æ–‡éƒ¨åˆ† -->

# LLM Interview All You Need

ğŸ“š Curating top company LLM interview questions with community collaboration!
ğŸš€ Continuously updating real questions from Google/OpenAI/Meta/Anthropic covering RAG, fine-tuning, deployment and more.

## Table of Contents

1. [Prompt Engineering & LLM Basics](#prompt-engineering--basics-of-llm)
2. [Retrieval Augmented Generation (RAG)](#retrieval-augmented-generation-rag)
3. [Document Digitization & Chunking](#document-digitization--chunking)
4. [Embedding Models](#embedding-models)
5. [Vector DB Internals](#internal-working-of-vector-databases)
6. [Advanced Search Algorithms](#advanced-search-algorithms)
7. [Language Model Internals](#language-models-internal-working)
8. [Supervised Fine-Tuning (SFT)](#supervised-fine-tuning-of-llm)
9. [Preference Alignment (RLHF/DPO)](#preference-alignment-rlhfdpo)
10. [Evaluating LLM Systems](#evaluation-of-llm-system)
11. [Hallucination Control](#hallucination-control-techniques)
12. [LLM Deployment Strategies](#deployment-of-llm)
13. [Agent-Based Systems](#agent-based-system)
14. [Prompt Hacking Defenses](#prompt-hacking)
15. [Miscellaneous Topics](#miscellaneous)
16. [Case Studies](#case-studies)

---

### Prompt Engineering & Basics

<details>
<summary>View Questions</summary>

- **What is the difference between Predictive/Discriminative AI and Generative AI?**
- **What is LLM, and how are LLMs trained?**
- **What is a token in the language model?**
- **How to estimate the cost of running SaaS-based and Open Source LLM models?**
- **Explain the Temperature parameter and how to set it.**
- **What are different decoding strategies for picking output tokens?**
- **What are different ways you can define stopping criteria in large language model?**
- **How to use stop sequences in LLMs?**
- **Explain the basic structure prompt engineering.**
- **Explain in-context learning**
- **Explain type of prompt engineering**
- **What are some of the aspect to keep in mind while using few-shots prompting?**
- **What are certain strategies to write good prompt?**
- **What is hallucination, and how can it be controlled using prompt engineering?**
- **How to improve the reasoning ability of LLM through prompt engineering?**
- **How to improve LLM reasoning if your COT prompt fails?**

[View Full Document â†’](/docs/prompt_engineering_basics)

</details>

### Retrieval Augmented Generation

<details>
<summary>View Questions</summary>

- **how to increase accuracy, and reliability & make answers verifiable in LLM**
- **How does RAG work?**
- **What are some benefits of using the RAG system?**
- **When should I use Fine-tuning instead of RAG?**
- **What are the architecture patterns for customizing LLM with proprietary data?**

[View Full Document â†’](/docs/rag_systems)

</details>

### Document Digitization & Chunking

<details>
<summary>View Questions</summary>

- **What is chunking, and why do we chunk our data?**
- **What factors influence chunk size?**
- **What are the different types of chunking methods?**
- **How to find the ideal chunk size?**
- **What is the best method to digitize and chunk complex documents like annual reports?**
- **How to handle tables during chunking?**
- **How do you handle very large table for better retrieval?**
- **How to handle list item during chunking?**
- **How do you build production grade document processing and indexing pipeline?**
- **How to handle graphs & charts in RAG**

[View Full Document â†’](/docs/document_processing)

</details>

### Embedding Models

<details>
<summary>View Questions</summary>

- **What are vector embeddings, and what is an embedding model?**
- **How is an embedding model used in the context of LLM applications?**
- **What is the difference between embedding short and long content?**
- **How to benchmark embedding models on your data?**
- **Suppose you are working with an open AI embedding model, after benchmarking accuracy is coming low, how would you further improve the accuracy of embedding the search model?**
- **Walk me through steps of improving sentence transformer model used for embedding?**

[View Full Document â†’](/docs/embedding_models)

</details>

### Vector DB Internals

<details>
<summary>View Questions</summary>

- **What is a vector database?**
- **Vector DB vs traditional DB**
- **Vector index vs DB vs plugins**
- **High-precision search strategy**
- **Clustering & LSH strategies**
- **Clustering mechanism and limits**
- **Random projection index**
- **Locality-sensitive hashing (LSH)**
- **Product quantization (PQ)**
- **Comparing vector indexes**
- **Selecting similarity metrics**
- **Filtering challenges**
- **Choosing vector databases**

[View Full Document â†’](/docs/vector_databases)

</details>

### Advanced Search Algorithms

<details>
<summary>View Questions</summary>

- **IR & semantic search patterns**
- **Importance of quality search**
- **Efficient large-scale search**
- **Improving inaccurate RAG retrieval**
- **Keyword-based retrieval**
- **Fine-tuning re-ranking models**
- **Common IR metrics & limitations**
- **Metric for Quora-like systems**
- **Recommendation system metrics**
- **IR metric comparison**
- **Hybrid search mechanics**
- **Merging multi-source results**
- **Multi-hop query handling**
- **Techniques to improve retrieval**

[View Full Document â†’](/docs/search_algorithms)

</details>

### LLM Internals

<details>
<summary>View Questions</summary>

- **Detailed self-attention explanation**
- **Disadvantages of self-attention**
- **Positional encoding explained**
- **Transformer architecture details**
- **Transformer advantages over LSTM**
- **Local vs global attention**
- **Transformer optimization techniques**
- **Extending LLM context length**
- **Optimizing for large vocabularies**
- **Balancing vocabulary size**
- **LLM architectures & best tasks**

[View Full Document â†’](/docs/llm_internals)

</details>

### Supervised Fine-Tuning

<details>
<summary>View Questions</summary>

- **Fine-tuning concepts & needs**
- **Scenarios requiring fine-tuning**
- **Making fine-tuning decisions**
- **Context-sufficient answering**
- **Creating Q&A datasets**
- **Hyperparameter configuration**
- **Infrastructure estimation**
- **Consumer hardware fine-tuning**
- **PEFT method categories**
- **Catastrophic forgetting**
- **Re-parameterized methods**

[View Full Document â†’](/docs/fine_tuning)

</details>

### Preference Alignment

<details>
<summary>View Questions</summary>

- **SFT vs preference alignment**
- **RLHF mechanisms & applications**
- **Reward hacking in RLHF**
- **Preference alignment methods**

[View Full Document â†’](/docs/preference_alignment)

</details>

### Evaluating LLM Systems

<details>
<summary>View Questions</summary>

- **Evaluating best LLM for task**
- **Assessing RAG systems**
- **Metrics for LLM evaluation**
- **Chain of Verification explained**

[View Full Document â†’](/docs/llm_evaluation)

</details>

### Hallucination Control

<details>
<summary>View Questions</summary>

- **Forms of hallucinations**
- **Controlling at various levels**

[View Full Document â†’](/docs/hallucination_control)

</details>

### LLM Deployment

<details>
<summary>View Questions</summary>

- **Why quantization preserves accuracy**
- **LLM inference throughput optimization**
- **Accelerating response time**

[View Full Document â†’](/docs/llm_deployment)

</details>

### Agent-Based Systems

<details>
<summary>View Questions</summary>

- **Agent concepts & strategies**
- **Need for agents & implementation**
- **ReAct prompting example**
- **Plan and Execute strategy**
- **OpenAI functions examples**
- **OpenAI vs LangChain Agents**

[View Full Document â†’](/docs/agent_systems)

</details>

### Prompt Hacking

<details>
<summary>View Questions</summary>

- **Prompt hacking explained**
- **Types of prompt hacking**
- **Defense tactics**

[View Full Document â†’](/docs/prompt_hacking)

</details>

### Miscellaneous

<details>
<summary>View Questions</summary>

- **Optimizing LLM system cost**
- **Mixture of Expert (MoE) models**
- **Building production RAG system**
- **FP8 advantages**
- **Low precision training**
- **KV cache size calculation**
- **Attention layer dimensions**
- **Focusing attention layers**

[View Full Document â†’](/docs/miscellaneous)

</details>

### Case Studies

<details>
<summary>View Questions</summary>

Coming soon

</details>

### ğŸ¤ How to Contribute

1. Add new questions to corresponding `.md` files
2. Improve existing answers (with citations)
3. Enhance documentation structure or translations
4. Share real interview experiences!

</div>
</div>


## æˆæƒåè®® (License)

ç‰ˆæƒæ‰€æœ‰ (c) 2025 llm_interview_all_you_need

ç‰¹æ­¤æˆäºˆä»»ä½•è·å¾—æœ¬è½¯ä»¶å‰¯æœ¬åŠç›¸å…³æ–‡æ¡£æ–‡ä»¶ï¼ˆä»¥ä¸‹ç®€ç§°"è½¯ä»¶"ï¼‰çš„ä»»ä½•äººå…è´¹è®¸å¯ï¼Œå…è®¸å…¶ä¸å—é™åˆ¶åœ°å¤„ç†æœ¬è½¯ä»¶ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºä½¿ç”¨ã€å¤åˆ¶ã€ä¿®æ”¹ã€åˆå¹¶ã€å‘å¸ƒã€åˆ†å‘ã€å†æˆæƒåŠ/æˆ–é”€å”®è½¯ä»¶å‰¯æœ¬çš„æƒåˆ©ï¼Œå¹¶å…è®¸è·å¾—è½¯ä»¶çš„äººè¿™æ ·åšï¼Œä½†é¡»æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

ä¸Šè¿°ç‰ˆæƒå£°æ˜å’Œæœ¬è®¸å¯å£°æ˜åº”åŒ…å«åœ¨è½¯ä»¶çš„æ‰€æœ‰å‰¯æœ¬æˆ–å®è´¨æ€§éƒ¨åˆ†ä¸­ã€‚

æœ¬è½¯ä»¶æŒ‰"åŸæ ·"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºé€‚é”€æ€§ä¿è¯ã€ç‰¹å®šç”¨é€”é€‚ç”¨æ€§ä¿è¯å’Œéä¾µæƒä¿è¯ã€‚åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œä½œè€…æˆ–ç‰ˆæƒæ‰€æœ‰è€…å‡ä¸å¯¹å› è½¯ä»¶æˆ–è½¯ä»¶ä½¿ç”¨æˆ–å…¶ä»–äº¤æ˜“è¡Œä¸ºè€Œäº§ç”Ÿçš„ä»»ä½•ç´¢èµ”ã€æŸå®³æˆ–å…¶ä»–è´£ä»»è´Ÿè´£ã€‚
